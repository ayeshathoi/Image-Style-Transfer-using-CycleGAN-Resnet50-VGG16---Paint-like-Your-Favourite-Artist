{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.models import Model\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "matplotlib.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path,dim1,dim2):\n",
    "    img= Image.open(image_path).resize((dim1,dim2)).convert(\"RGB\")\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image,title=None):\n",
    "    if len(image.shape)>3:\n",
    "        image=tf.squeeze(image,axis=0)\n",
    "    plt.imshow(image) \n",
    "    if title:\n",
    "        plt.title=title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid(images,num_rows=1):\n",
    "    n=len(images)\n",
    "    if n > 1:\n",
    "        num_cols=np.ceil(n/num_rows)\n",
    "        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))\n",
    "        axes=axes.flatten()\n",
    "        fig.set_size_inches((20,20))\n",
    "        for i,image in enumerate(images):\n",
    "            axes[i].axis('off')\n",
    "            axes[i].imshow(image)\n",
    "    else:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModel:\n",
    "    def __init__(self):\n",
    "        self.model= vgg16.VGG16(weights='imagenet',include_top=False)\n",
    "        self.content_layers=  ['block3_conv3']\n",
    "        self.style_layers= ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "        self.loss_model=self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model.trainable=False\n",
    "        # Feature extraction model\n",
    "        new_model=Model(inputs=self.model.input,outputs=[self.model.get_layer(name).output for name in (self.style_layers+self.content_layers)])\n",
    "        return new_model\n",
    "    \n",
    "    def get_activations(self,inputs):\n",
    "        inputs=inputs*255.0 # so that the input is in the same range as the images used to train VGG\n",
    "        style_length=len(self.style_layers)  # so that we can split the outputs into style and content\n",
    "        outputs=self.loss_model(vgg16.preprocess_input(inputs)) # preprocess_input is used to normalize the input\n",
    "        style_output,content_output=outputs[:style_length],outputs[style_length:] # split the outputs into style and content\n",
    "        content_dict={name:value for name,value in zip(self.content_layers,content_output)} # create a dictionary with the content layers\n",
    "        style_dict={name:value for name,value in zip(self.style_layers,style_output)} # create a dictionary with the style layers\n",
    "        return {'content':content_dict,'style':style_dict} # return the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model=LossModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_weights=[1]\n",
    "style_layers_weights=[1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(placeholder,content,weight):\n",
    "    return weight*tf.reduce_mean(tf.square(placeholder-content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x) # calculate the gram matrix using einsum \n",
    "    # Einstein summation is a compact representation for combining products and sums in a general way\n",
    "    return gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(placeholder,style,weight):\n",
    "    s=gram_matrix(style)\n",
    "    p=gram_matrix(placeholder)\n",
    "    return weight*tf.reduce_mean(tf.square(s-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preceptual_loss(predicted_activations,content_activations,style_activations,content_weight,style_weight,content_layers_weights,style_layer_weights):\n",
    "    pred_content=predicted_activations[\"content\"]\n",
    "    pred_style=predicted_activations[\"style\"]\n",
    "    c_loss=tf.add_n([content_loss(pred_content[name],content_activations[name],content_layers_weights[i]) for i,name in enumerate(pred_content.keys())])\n",
    "    c_loss=c_loss*content_weight\n",
    "    s_loss=tf.add_n([style_loss(pred_style[name],style_activations[name],style_layer_weights[i]) for i,name in enumerate(pred_style.keys())])\n",
    "    s_loss=s_loss*style_weight\n",
    "    return c_loss+s_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.padding = tuple(padding)\n",
    "    def call(self, input_tensor):\n",
    "        padding_width, padding_height = self.padding\n",
    "        return tf.pad(input_tensor, [[0,0], [padding_height, padding_height], [padding_width, padding_width], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "    def call(self,inputs):\n",
    "        channels=inputs.shape[-1]\n",
    "        batch, rows, cols, channels = [i for i in inputs.get_shape()] \n",
    "        mu, var = tf.nn.moments(inputs, [1,2], keepdims=True)\n",
    "        shift = tf.Variable(tf.zeros([channels]))\n",
    "        scale = tf.Variable(tf.ones([channels]))\n",
    "        epsilon = 1e-3\n",
    "        normalized = (inputs-mu)/tf.sqrt(var + epsilon)\n",
    "        return scale * normalized + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,filters,kernel_size,strides=1,**kwargs):\n",
    "        super(ConvLayer,self).__init__(**kwargs)\n",
    "        self.padding=ReflectionPadding2D([k//2 for k in kernel_size])\n",
    "        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n",
    "        self.bn=InstanceNormalization()\n",
    "    def call(self,inputs):\n",
    "        x=self.padding(inputs)\n",
    "        x=self.conv2d(x)\n",
    "        x=self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,filters,kernel_size,**kwargs):\n",
    "        super(ResidualLayer,self).__init__(**kwargs)\n",
    "        self.conv2d_1=ConvLayer(filters,kernel_size)\n",
    "        self.conv2d_2=ConvLayer(filters,kernel_size)\n",
    "        self.relu=tf.keras.layers.ReLU()\n",
    "        self.add=tf.keras.layers.Add()\n",
    "    def call(self,inputs):\n",
    "        residual=inputs\n",
    "        x=self.conv2d_1(inputs)\n",
    "        x=self.relu(x)\n",
    "        x=self.conv2d_2(x)\n",
    "        x=self.add([x,residual])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleLayer(tf.keras.layers.Layer):\n",
    "    #Upsampling means increasing the dimensions of the image by a factor\n",
    "    def __init__(self,filters,kernel_size,strides=1,upsample=2,**kwargs):\n",
    "        super(UpsampleLayer,self).__init__(**kwargs)\n",
    "        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)\n",
    "        self.padding=ReflectionPadding2D([k//2 for k in kernel_size])\n",
    "        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n",
    "        self.bn=InstanceNormalization()\n",
    "    def call(self,inputs):\n",
    "        x=self.upsample(inputs)\n",
    "        x=self.padding(x)\n",
    "        x=self.conv2d(x)\n",
    "        return self.bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    def __init__(self,**kwargs):\n",
    "        #Convolutional layers are used to extract features from the input image\n",
    "        #Residual layers are used to learn the features\n",
    "        #Upsampling layers are used to increase the dimensions of the image\n",
    "        #Reflection padding is used to pad the image with the reflection of the image\n",
    "        #Reflection of an image is the mirror image of the original image\n",
    "        #Instance normalization is used to normalize the image\n",
    "        #tanH is used to scale the image to the range of 0 to 255\n",
    "\n",
    "        \n",
    "        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)\n",
    "        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name=\"conv2d_1_32\")\n",
    "        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name=\"conv2d_2_64\")\n",
    "        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name=\"conv2d_3_128\")\n",
    "        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_1_128\")\n",
    "        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_2_128\")\n",
    "        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_3_128\")\n",
    "        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_4_128\")\n",
    "        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_5_128\")\n",
    "        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name=\"deconv2d_1_64\")\n",
    "        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name=\"deconv2d_2_32\")\n",
    "        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name=\"deconv2d_3_3\")\n",
    "        self.relu=tf.keras.layers.ReLU()\n",
    "    def call(self, inputs):\n",
    "        x=self.conv2d_1(inputs)\n",
    "        x=self.relu(x)\n",
    "        x=self.conv2d_2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.conv2d_3(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.res_1(x)\n",
    "        x=self.res_2(x)\n",
    "        x=self.res_3(x)\n",
    "        x=self.res_4(x)\n",
    "        x=self.res_5(x)\n",
    "        x=self.deconv2d_1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.deconv2d_2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.deconv2d_3(x)\n",
    "        x = (tf.nn.tanh(x) + 1) * (255.0 / 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,checkpoint_path=\"./\",content_weight=1e4,style_weight=1e-2,total_variation_weight=0.004,content_layers_weights=[1],style_layers_weights=[1]*5):\n",
    "    batch_losses=[]\n",
    "    steps=1\n",
    "    save_path=os.path.join(checkpoint_path,f\"model_checkpoint.ckpt\")\n",
    "    print(\"Model Checkpoint Path: \",save_path)\n",
    "    for input_image_batch in dataset:\n",
    "        if steps-1 >= steps_per_epoch:\n",
    "            break\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs=style_model(input_image_batch)\n",
    "            outputs=tf.clip_by_value(outputs, 0, 255)\n",
    "            pred_activations=loss_model.get_activations(outputs/255.0)\n",
    "            content_activations=loss_model.get_activations(input_image_batch)[\"content\"] \n",
    "            curr_loss=preceptual_loss(pred_activations,content_activations,style_activations,content_weight,\n",
    "                                      style_weight,content_layers_weights,style_layers_weights)\n",
    "            curr_loss += total_variation_weight*tf.image.total_variation(outputs)\n",
    "        batch_losses.append(curr_loss)\n",
    "        grad = tape.gradient(curr_loss,style_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))\n",
    "        if steps%100==0:\n",
    "            print(\"checkpoint saved \",end=\" \")\n",
    "            style_model.save_weights(save_path)\n",
    "            print(f\"Loss: {tf.reduce_mean(batch_losses).numpy()}\")\n",
    "        steps+=1\n",
    "    return tf.reduce_mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorflowDatasetLoader:\n",
    "    def __init__(self,dataset_path,batch_size=4, image_size=(256, 256)):\n",
    "        images_paths = [str(path) for path in Path(dataset_path).glob(\"*.jpg\")]  # get the paths of the images\n",
    "        self.length=len(images_paths) # store the length of the dataset\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(\n",
    "            lambda path: self.load_tf_image(path, dim=image_size), # load the images\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE, # parallelize the map function so that the images are loaded faster\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size,drop_remainder=True) # batch the dataset\n",
    "        dataset = dataset.repeat() # repeat the dataset\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) # prefetch the dataset so that the next batch is ready to be used\n",
    "        self.dataset=dataset # store the dataset\n",
    "    def __len__(self):\n",
    "        return self.length # return the length of the dataset\n",
    "    \n",
    "    @tf.function\n",
    "    def load_tf_image(self,image_path,dim): \n",
    "        image = tf.io.read_file(image_path) # read the image\n",
    "        image = tf.image.decode_jpeg(image, channels=3) # decode the image\n",
    "        image= tf.image.resize(image,dim) # resize the image\n",
    "        image= image/255.0 # normalize the image\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32) # convert the image to float32\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"d:/Study Materials/4-2/paint like any painter/Image_data/photo_jpg/\" \n",
    "style_path = \"d:/Study Materials/4-2/paint like any painter/Image_data/monet_jpg/\"\n",
    "\n",
    "print(style_path)\n",
    "print(input_path)\n",
    "\n",
    "\n",
    "\n",
    "input_shape=(256,256,3)\n",
    "batch_size=4\n",
    "style_model = StyleTransferModel()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "loader=TensorflowDatasetLoader(input_path,batch_size=4)\n",
    "plot_images_grid(next(iter(loader.dataset.take(1))),num_rows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_path =os.path.join(style_path,\"d239dae42d.jpg\")\n",
    "style_index = \"d239dae42d\"\n",
    "style_image=load_image(style_image_path,input_shape[0],input_shape[1])\n",
    "style_image=style_image/255.0\n",
    "\n",
    "show_image(style_image)\n",
    "\n",
    "style_image=style_image.astype(np.float32)\n",
    "style_image_batch=np.repeat([style_image],batch_size,axis=0)\n",
    "style_activations=loss_model.get_activations(style_image_batch)[\"style\"]\n",
    "\n",
    "\n",
    "epochs=10\n",
    "content_weight = 20\n",
    "style_weight=1e2\n",
    "total_variation_weight=0.004\n",
    "\n",
    "num_images=len(loader)\n",
    "steps_per_epochs=num_images//batch_size\n",
    "print(steps_per_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path=\"model_checkpoint\"\n",
    "os.makedirs(model_save_path,exist_ok=True)\n",
    "\n",
    "try: \n",
    "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.experimental.set_policy(policy) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    tf.config.optimizer.set_jit(True) # \n",
    "except:\n",
    "    pass\n",
    "\n",
    "if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n",
    "    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(\"resuming training ...\")\n",
    "else:\n",
    "    print(\"training scratch ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses=[]\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,style_model,loss_model,optimizer,\n",
    "                          model_save_path,\n",
    "                          content_weight,style_weight,total_variation_weight,\n",
    "                          content_layers_weights,style_layers_weights)\n",
    "    style_model.save_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(\"Model Checkpointed at: \",os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(f\"loss: {batch_loss.numpy()}\")\n",
    "    epoch_losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n",
    "    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(\"loading weights ...\")\n",
    "else:\n",
    "    print(\"no weights found ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_images = []\n",
    "\n",
    "for images in loader.dataset.take(1):\n",
    "    images = images * 255.0\n",
    "    generated_images = style_model(images)\n",
    "    generated_images = np.clip(generated_images,0,255)\n",
    "    generated_images = generated_images.astype(np.uint8)\n",
    "    for image in generated_images:\n",
    "        styled_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_grid(next(iter(loader.dataset.take(1))),num_rows = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_grid(styled_images,num_rows = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m\"\u001b[39m\u001b[39m../\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m style_path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgenerated\u001b[39m\u001b[39m\"\u001b[39m,exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m dir_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m style_path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgenerated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../\" + style_path.split(\"/\")[-1] + \"generated\",exist_ok=True)\n",
    "dir_name = \"../\" + style_path.split(\"/\")[-1] + \"generated\"\n",
    "\n",
    "i = 0\n",
    "for images in loader.dataset.take(75):\n",
    "    images = images * 255.0\n",
    "    generated_images = style_model(images)\n",
    "    generated_images = np.clip(style_model(images),0,255).astype(np.uint8)\n",
    "    for image in generated_images:\n",
    "        img = Image.fromarray(image)\n",
    "        img.save(os.path.join(dir_name,f\"{i}.jpg\"))\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
